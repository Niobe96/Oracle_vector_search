import streamlit as st
from PIL import Image
import torch
import torchvision.models as models
import torchvision.transforms as transforms
import cv2
import numpy as np
import faiss
import pickle
import os
from ultralytics import YOLO # YOLOv8 ì´ìƒì„ ìœ„í•œ import

# --- âš™ï¸ 1. ì„¤ì • (Configuration) ---
# í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì—¬ OMP ì˜¤ë¥˜ ê²½ê³ ë¥¼ ë¹„í™œì„±í™” (ê°€ì¥ ìœ„ì— ìœ„ì¹˜)
os.environ['KMP_DUPLICATE_LIB_OK']='True'

# app.py íŒŒì¼ì˜ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ì„¤ì •
BASE_PROJECT_DIR = os.path.dirname(os.path.abspath(__file__))
ASSETS_DIR = os.path.join(BASE_PROJECT_DIR, 'assets')
OUTPUT_DIR = os.path.join(BASE_PROJECT_DIR, 'output')

# ì‚¬ìš©í•  íŒŒì¼ ê²½ë¡œë“¤
YOLO_MODEL_PATH = os.path.join(ASSETS_DIR, 'best.pt')
FAISS_INDEX_PATH = os.path.join(ASSETS_DIR, 'tumor_db.index')
METADATA_PKL_PATH = os.path.join(ASSETS_DIR, 'metadata.pkl')

# data.yaml íŒŒì¼ì— ì •ì˜ëœ í´ë˜ìŠ¤ ì´ë¦„ ìˆœì„œì™€ ì¼ì¹˜í•´ì•¼ í•¨
CLASS_NAMES = ['glioma tumor', 'meningioma tumor', 'no tumor', 'pituitary tumor'] 

# --- ğŸ§  2. ëª¨ë¸ ë° DB ë¡œë”© (Streamlit ìºì‹± ê¸°ëŠ¥ ì‚¬ìš©) ---
@st.cache_resource
def load_all():
    """AI ëª¨ë¸, FAISS DB, ë©”íƒ€ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. ì•± ì‹¤í–‰ ì‹œ í•œ ë²ˆë§Œ í˜¸ì¶œë©ë‹ˆë‹¤."""
    print("Loading models and DB for local execution...")
    
    # 1. YOLOv8 ëª¨ë¸ ë¡œë“œ (ìµœì‹  ë°©ì‹)
    yolo_model = YOLO(YOLO_MODEL_PATH)
    
    # 2. ResNet-18 íŠ¹ì§• ì¶”ì¶œê¸° ë¡œë“œ
    device = "cuda" if torch.cuda.is_available() else "cpu"
    resnet_model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
    resnet_model.fc = torch.nn.Identity()
    resnet_model.eval()
    resnet_model.to(device)

    # 3. FAISS ì¸ë±ìŠ¤ ë¡œë“œ
    faiss_index = faiss.read_index(FAISS_INDEX_PATH)

    # 4. ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸ ë¡œë“œ
    with open(METADATA_PKL_PATH, "rb") as f:
        metadata_list = pickle.load(f)
    
    print("Loading complete.")
    return yolo_model, resnet_model, faiss_index, metadata_list, device

# --- ğŸ› ï¸ 3. í—¬í¼ í•¨ìˆ˜ ì •ì˜ ---
def apply_clahe_to_image(image_bgr):
    """OpenCV BGR ì´ë¯¸ì§€ì— CLAHEë¥¼ ì ìš©í•©ë‹ˆë‹¤."""
    gray_img = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)
    clahe = cv2.createCLAHE(clipLimit=4.0, tileGridSize=(8, 8))
    clahe_gray_img = clahe.apply(gray_img)
    clahe_bgr_img = cv2.cvtColor(clahe_gray_img, cv2.COLOR_GRAY2BGR)
    return clahe_bgr_img

def extract_features_from_array(image_array, model, device):
    """CNNìœ¼ë¡œ ì´ë¯¸ì§€ ë°°ì—´ì—ì„œ íŠ¹ì§• ë²¡í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    preprocessor = transforms.Compose([
        transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])
    img = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)
    img = Image.fromarray(img)
    img_t = preprocessor(img)
    batch_t = torch.unsqueeze(img_t, 0).to(device)
    with torch.no_grad():
        features = model(batch_t)
    return features.squeeze().cpu().numpy()

# --- ğŸ–¥ï¸ 4. Streamlit UI ë° ë©”ì¸ ë¡œì§ ---
st.set_page_config(layout="wide")
st.title("ğŸ§  ë‡Œì¢…ì–‘ ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ ì‹œìŠ¤í…œ")
st.write("MRI ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ë©´, AIê°€ ì¢…ì–‘ì„ íƒì§€í•˜ê³  DBì—ì„œ ê°€ì¥ ìœ ì‚¬í•œ ì‚¬ë¡€ë¥¼ ì°¾ì•„ ë³´ì—¬ì¤ë‹ˆë‹¤.")

try:
    yolo, cnn, index, metadata_list, device = load_all()
except Exception as e:
    st.error(f"ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    st.info("assets í´ë”ì— í•„ìš”í•œ íŒŒì¼ë“¤(best.pt, tumor_db.index, metadata.pkl)ì´ ìˆëŠ”ì§€, ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

uploaded_file = st.file_uploader("MRI ì´ë¯¸ì§€ë¥¼ ì—¬ê¸°ì— ì—…ë¡œë“œí•˜ì„¸ìš”...", type=["jpg", "jpeg", "png"])

if uploaded_file is not None:
    file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)
    original_image = cv2.imdecode(file_bytes, 1)
    
    st.image(original_image, channels="BGR", caption="ì—…ë¡œë“œëœ ì›ë³¸ ì´ë¯¸ì§€", width=300)

    with st.spinner('AIê°€ ì´ë¯¸ì§€ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...'):
        # 1. CLAHE ì „ì²˜ë¦¬
        clahe_image = apply_clahe_to_image(original_image)
        
        # 2. YOLOv8ë¡œ ì¢…ì–‘ íƒì§€
        results = yolo(clahe_image, verbose=False) # verbose=Falseë¡œ í„°ë¯¸ë„ ë¡œê·¸ ê¹”ë”í•˜ê²Œ
        result = results[0] 

        # 3. ê²°ê³¼ ì²˜ë¦¬
        if len(result.boxes) == 0:
            st.warning("ì´ë¯¸ì§€ì—ì„œ ì¢…ì–‘ì´ íƒì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        else:
            # ê°€ì¥ ì‹ ë¢°ë„ ë†’ì€ íƒì§€ ê²°ê³¼ í•˜ë‚˜ë§Œ ì‚¬ìš©
            best_box_index = result.boxes.conf.argmax()
            best_box = result.boxes[best_box_index]
            
            x1, y1, x2, y2 = map(int, best_box.xyxy[0])
            confidence = float(best_box.conf[0])
            
            # 4. ì¢…ì–‘ ì˜ì—­ ìë¥´ê¸° ë° íŠ¹ì§• ì¶”ì¶œ
            cropped_tumor = clahe_image[y1:y2, x1:x2]
            query_vector = extract_features_from_array(cropped_tumor, cnn, device)
            
            st.write("---")
            col1, col2 = st.columns([1, 3])
            with col1:
                st.image(cropped_tumor, channels="BGR", caption="íƒì§€ ë° ë¶„ì„ëœ ì¢…ì–‘")
            with col2:
                st.info(f"ìœ„ ì¢…ì–‘(ì‹ ë¢°ë„: {confidence:.2f})ì˜ íŠ¹ì§•ì„ ë¶„ì„í•˜ì—¬ DBì™€ ë¹„êµí•©ë‹ˆë‹¤.")

            # 5. FAISSë¡œ ìœ ì‚¬ ì´ë¯¸ì§€ ê²€ìƒ‰
            query_vector_np = np.array([query_vector]).astype('float32')
            faiss.normalize_L2(query_vector_np)
            
            k = 5
            distances, indices = index.search(query_vector_np, k)

            # 6. ê²°ê³¼ ì‹œê°í™”
            st.write("---")
            st.subheader("ê°€ì¥ ìœ ì‚¬í•œ ì¢…ì–‘ ì‚¬ë¡€ Top 5")
            
            indices_list = indices[0]
            for i, result_idx in enumerate(indices_list):
                result_meta = metadata_list[result_idx]
                class_id = result_meta['class_id']
                class_name = CLASS_NAMES[class_id].title()
                similarity_score = distances[0][i]
                
                with st.expander(f"Rank {i+1}: **{class_name}** (ìœ ì‚¬ë„: {similarity_score:.3f})", expanded=(i < 2)):
                    exp_col1, exp_col2 = st.columns(2)
                    
                    with exp_col1:
                        # ì˜ë¼ë‚¸ ì¢…ì–‘ ì´ë¯¸ì§€ ë³´ì—¬ì£¼ê¸°
                        cropped_img_path = result_meta['cropped_path']
                        if os.path.exists(cropped_img_path):
                            image = Image.open(cropped_img_path)
                            st.image(image, caption="ìœ ì‚¬í•œ ì¢…ì–‘ ë¶€ìœ„ (Cropped)")
                        else:
                            st.warning(f"ì˜ë¼ë‚¸ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {cropped_img_path}")
                            
                    with exp_col2:
                        # ì›ë³¸ ì „ì²´ ì´ë¯¸ì§€ì— ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë ¤ì„œ ë³´ì—¬ì£¼ê¸°
                        original_img_path = result_meta['original_path']
                        if os.path.exists(original_img_path):
                            original_image_with_box = cv2.imread(original_img_path)
                            coords = result_meta['coords']
                            b_x1, b_y1, b_x2, b_y2 = coords
                            cv2.rectangle(original_image_with_box, (b_x1, b_y1), (b_x2, b_y2), (0, 0, 255), 2)
                            st.image(original_image_with_box, channels="BGR", caption="ì „ì²´ ì›ë³¸ MRI (With BBox)")
                        else:
                            st.warning(f"ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {original_img_path}")